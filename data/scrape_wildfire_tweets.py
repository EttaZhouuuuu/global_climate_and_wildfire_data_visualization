#!/usr/bin/env python3
"""
Twitter Wildfire Sentiment Analysis Scraper
‰ΩøÁî® Snscrape Áà¨ÂèñÂä†Â∑ûÂ§ßÁÅ´Áõ∏ÂÖ≥Êé®ÊñáÔºåËøõË°åÊÉÖÊÑüÂàÜÊûêÔºåÁîüÊàêËØç‰∫ëÊâÄÈúÄÊï∞ÊçÆ

‰ΩøÁî®ÊñπÊ≥ï:
    python3 scrape_wildfire_tweets.py

Ê≥®ÊÑè‰∫ãÈ°π:
    - macOS ‰∏äËØ∑‰ΩøÁî® python3 ËÄå‰∏çÊòØ python
    - ÈúÄË¶Å VPN ÊâçËÉΩËÆøÈóÆ Twitter/X
    - Â¶ÇÊûúÊó†Ê≥ïËÆøÈóÆÔºåÂèØ‰ª•ÊâãÂä®ÂàõÂª∫ sentiment_analysis.csv
"""

import snscrape.modules.twitter as sntwitter
import pandas as pd
import re
from collections import Counter
from textblob import TextBlob
import nltk
from datetime import datetime, timedelta
import json
import os

# ‰∏ãËΩΩÂøÖË¶ÅÁöÑ NLTK Êï∞ÊçÆ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)

try:
    from nltk.corpus import stopwords
    STOPWORDS = set(stopwords.words('english'))
except:
    STOPWORDS = set()

# Ê∑ªÂä†Ëá™ÂÆö‰πâÂÅúÁî®ËØçÔºàÂ∏∏ËßÅ‰ΩÜÊó†ÊÑè‰πâÁöÑËØçÔºâ
CUSTOM_STOPWORDS = {
    'rt', 'https', 'http', 'co', 'amp', 'via', 'new', 'get', 'got',
    'one', 'two', 'would', 'could', 'like', 'just', 'really', 'still',
    'going', 'want', 'know', 'think', 'see', 'look', 'make', 'let'
}
STOPWORDS = STOPWORDS.union(CUSTOM_STOPWORDS)

# Êâ©Â±ïÂÅúÁî®ËØçÂàóË°®ÔºàÈíàÂØπÈáéÁÅ´Êé®ÊñáÁâπÂÆöÔºâ
WILDFIRE_STOPWORDS = {
    'wildfire', 'wildfires', 'fire', 'fires', 'california', 'ca',
    'rt', 'https', 'http', 'co', 'amp', 'via', 'new', 'get', 'got',
    'one', 'two', 'would', 'could', 'like', 'just', 'really', 'still',
    'going', 'want', 'know', 'think', 'see', 'look', 'make', 'let',
    'today', 'day', 'week', 'month', 'time', 'year', 'years',
    'people', 'person', 'thing', 'stuff', 'lot', 'much', 'many',
    'im', 'ive', 'dont', 'cant', 'wont', 'didnt', 'thats', 'youre',
    'us', 'our', 'weve', 'theyre', 'hes', 'shes', 'its', 'heres'
}
STOPWORDS = STOPWORDS.union(WILDFIRE_STOPWORDS)


def clean_tweet(tweet_text):
    """
    Ê∏ÖÁêÜÊé®ÊñáÊñáÊú¨
    - ÁßªÈô§ URLs
    - ÁßªÈô§ @mentions
    - ÁßªÈô§ÁâπÊÆäÂ≠óÁ¨¶
    - ËΩ¨Â∞èÂÜô
    """
    if not tweet_text:
        return ""
    
    # ÁßªÈô§ URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    # ÁßªÈô§ @mentions
    text = re.sub(r'@\w+', '', text)
    # ÁßªÈô§ # Á¨¶Âè∑‰ΩÜ‰øùÁïôÊ†áÁ≠æÊñáÂ≠ó
    text = re.sub(r'#', '', text)
    # Âè™‰øùÁïôÂ≠óÊØç
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # ËΩ¨Â∞èÂÜô
    text = text.lower()
    # ÁßªÈô§Â§ö‰ΩôÁ©∫Ê†º
    text = ' '.join(text.split())
    
    return text


def extract_words(text):
    """
    ÊèêÂèñÊúâÊÑè‰πâÁöÑËØçËØ≠
    - ÁßªÈô§ÂÅúÁî®ËØç
    - Âè™‰øùÁïôÈïøÂ∫¶ > 2 ÁöÑËØç
    """
    if not text:
        return []
    
    words = text.split()
    meaningful_words = [
        word for word in words 
        if word not in STOPWORDS 
        and len(word) > 2
    ]
    
    return meaningful_words


def analyze_sentiment(text):
    """
    ‰ΩøÁî® TextBlob ÂàÜÊûêÊÉÖÊÑü
    ËøîÂõû: 1 (Ê≠£Èù¢), 0 (‰∏≠ÊÄß), -1 (Ë¥üÈù¢)
    """
    if not text:
        return 0
    
    try:
        blob = TextBlob(str(text))
        polarity = blob.sentiment.polarity
        
        if polarity > 0.1:
            return 1   # Ê≠£Èù¢
        elif polarity < -0.1:
            return -1  # Ë¥üÈù¢
        else:
            return 0   # ‰∏≠ÊÄß
    except:
        return 0


def get_sentiment_label(sentiment):
    """
    Ëé∑ÂèñÊÉÖÊÑüÊ†áÁ≠æ
    """
    if sentiment == 1:
        return "Positive"
    elif sentiment == -1:
        return "Negative"
    else:
        return "Neutral"


def scrape_california_wildfire_tweets(max_tweets=5000):
    """
    Áà¨ÂèñÂä†Â∑ûÂ§ßÁÅ´Áõ∏ÂÖ≥Êé®Êñá
    """
    
    print("=" * 60)
    print("üê¶ ÂºÄÂßãÁà¨ÂèñÂä†Â∑ûÂ§ßÁÅ´Êé®Êñá...")
    print("=" * 60)
    
    # ÂÆö‰πâÊêúÁ¥¢ÂÖ≥ÈîÆËØç
    search_queries = [
        '(#CAfire OR #CaliforniaFire OR "California wildfire" OR "CA fire") lang:en',
        '(#DixieFire OR #CaldorFire OR #AugustComplexFire) lang:en',
        '("wildfire" OR "forest fire") California -is:retweet lang:en',
        '(#LAFires OR #SoCalFires OR "Southern California fire") lang:en',
        '("evacuation" OR "evacuate" OR "emergency") wildfire California lang:en',
        '("destroyed" OR "burned" OR "burning") California fire lang:en',
    ]
    
    all_tweets = []
    
    for query in search_queries:
        print(f"\nüìä ÊêúÁ¥¢ÂÖ≥ÈîÆËØç: {query[:60]}...")
        
        tweets_list = []
        
        try:
            for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):
                if len(all_tweets) >= max_tweets:
                    break
                    
                if tweet.content and len(tweet.content) > 20:
                    tweets_list.append({
                        'date': tweet.date,
                        'username': tweet.user.username,
                        'content': tweet.content,
                        'url': tweet.url,
                        'reply_count': tweet.replyCount,
                        'retweet_count': tweet.retweetCount,
                        'like_count': tweet.likeCount
                    })
                    
                if (i + 1) % 100 == 0:
                    print(f"   Â∑≤Ëé∑Âèñ {i + 1} Êù°Êé®Êñá...")
                    
                if len(tweets_list) >= max_tweets // len(search_queries) + 1:
                    break
                    
        except Exception as e:
            print(f"   ‚ö†Ô∏è ÊêúÁ¥¢Âá∫Èîô: {e}")
            continue
        
        all_tweets.extend(tweets_list)
        print(f"‚úì ÂÆåÊàêÊêúÁ¥¢ÔºåËé∑Âèñ {len(tweets_list)} Êù°Êé®Êñá")
    
    print(f"\nüìà ÊÄªËÆ°Ëé∑Âèñ {len(all_tweets)} Êù°Êé®Êñá")
    
    return all_tweets


def process_tweets(tweets):
    """
    Â§ÑÁêÜÊé®ÊñáÔºöÊ∏ÖÁêÜ„ÄÅÂàÜÊûêÊÉÖÊÑü„ÄÅÁªüËÆ°ËØçÈ¢ë
    """
    print("\n" + "=" * 60)
    print("üîç Â§ÑÁêÜÊé®ÊñáÊï∞ÊçÆ...")
    print("=" * 60)
    
    word_counts = Counter()
    word_sentiments = {}
    tweet_data = []
    
    for i, tweet in enumerate(tweets):
        if i % 500 == 0:
            print(f"   Â§ÑÁêÜËøõÂ∫¶: {i}/{len(tweets)}")
        
        # Ê∏ÖÁêÜÊé®Êñá
        cleaned_text = clean_tweet(tweet['content'])
        words = extract_words(cleaned_text)
        
        # ÂàÜÊûêÊÉÖÊÑü
        sentiment = analyze_sentiment(tweet['content'])
        
        # ÁªüËÆ°ËØçÈ¢ë
        for word in words:
            word_counts[word] += 1
            if word not in word_sentiments:
                word_sentiments[word] = []
            word_sentiments[word].append(sentiment)
        
        tweet_data.append({
            'date': tweet['date'],
            'content': tweet['content'],
            'sentiment': sentiment,
            'cleaned_text': cleaned_text
        })
    
    # ËÆ°ÁÆóÊØè‰∏™ËØçÁöÑÂπ≥ÂùáÊÉÖÊÑü
    word_sentiment_avg = {}
    for word, sentiments in word_sentiments.items():
        avg = sum(sentiments) / len(sentiments)
        if avg > 0.1:
            word_sentiment_avg[word] = 1
        elif avg < -0.1:
            word_sentiment_avg[word] = -1
        else:
            word_sentiment_avg[word] = 0
    
    print(f"   ÊèêÂèñ‰∫Ü {len(word_counts)} ‰∏™‰∏çÂêåÁöÑËØç")
    
    return word_counts, word_sentiment_avg, tweet_data


def generate_sentiment_csv(word_counts, word_sentiments, output_file):
    """
    ÁîüÊàê sentiment_analysis.csv Êñá‰ª∂
    """
    print("\n" + "=" * 60)
    print("üìÅ ÁîüÊàê CSV Êñá‰ª∂...")
    print("=" * 60)
    
    # Ëé∑ÂèñÂâç100‰∏™È´òÈ¢ëËØç
    top_words = word_counts.most_common(100)
    
    # ÂàõÂª∫ DataFrame
    data = []
    for word, frequency in top_words:
        sentiment = word_sentiments.get(word, 0)
        data.append({
            'Word': word,
            'Frequency': frequency,
            'Sentiment': sentiment
        })
    
    df = pd.DataFrame(data)
    df.to_csv(output_file, index=False)
    
    print(f"‚úÖ Â∑≤ÁîüÊàê {output_file}")
    print(f"   ÂåÖÂê´ {len(df)} ‰∏™ËØç")
    
    # ÊòæÁ§∫ÁªüËÆ°‰ø°ÊÅØ
    positive = len(df[df['Sentiment'] == 1])
    neutral = len(df[df['Sentiment'] == 0])
    negative = len(df[df['Sentiment'] == -1])
    
    print(f"\nüìä ÊÉÖÊÑüÂàÜÂ∏É:")
    print(f"   üòä Ê≠£Èù¢ËØç: {positive} ({positive/len(df)*100:.1f}%)")
    print(f"   üòê ‰∏≠ÊÄßËØç: {neutral} ({neutral/len(df)*100:.1f}%)")
    print(f"   üòû Ë¥üÈù¢ËØç: {negative} ({negative/len(df)*100:.1f}%)")
    
    return df


def generate_tweets_csv(tweet_data, output_file):
    """
    ÁîüÊàêÂ§ÑÁêÜÂêéÁöÑÂÆåÊï¥Êé®ÊñáÊï∞ÊçÆ CSV
    """
    df = pd.DataFrame(tweet_data)
    df.to_csv(output_file, index=False)
    print(f"‚úÖ Â∑≤ÁîüÊàê {output_file}")
    print(f"   ÂåÖÂê´ {len(df)} Êù°Êé®Êñá")
    
    return df


def print_top_words(word_counts, word_sentiments, n=20):
    """
    ÊâìÂç∞È´òÈ¢ëËØçÂàóË°®
    """
    print("\n" + "=" * 60)
    print(f"üîù Top {n} È´òÈ¢ëËØç:")
    print("=" * 60)
    
    top_n = word_counts.most_common(n)
    
    print(f"{'ÊéíÂêç':<5} {'ËØçËØ≠':<15} {'È¢ëÊ¨°':<8} {'ÊÉÖÊÑü':<10}")
    print("-" * 40)
    
    for i, (word, count) in enumerate(top_n, 1):
        sentiment = word_sentiments.get(word, 0)
        sentiment_label = get_sentiment_label(sentiment)
        emoji = "üòä" if sentiment == 1 else "üòê" if sentiment == 0 else "üòû"
        print(f"{i:<5} {word:<15} {count:<8} {emoji} {sentiment_label}")
    
    return top_n


def main():
    """
    ‰∏ªÂáΩÊï∞
    """
    # ÈÖçÁΩÆ
    MAX_TWEETS = 3000  # ÊúÄÂ§ßÊé®ÊñáÊï∞Èáè
    OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))
    DATA_DIR = os.path.join(OUTPUT_DIR, 'data')
    
    # Á°Æ‰øùËæìÂá∫ÁõÆÂΩïÂ≠òÂú®
    os.makedirs(DATA_DIR, exist_ok=True)
    
    OUTPUT_CSV = os.path.join(DATA_DIR, 'sentiment_analysis.csv')
    TWEETS_CSV = os.path.join(DATA_DIR, 'wildfire_tweets_raw.csv')
    
    # Êó∂Èó¥Êà≥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    BACKUP_DIR = os.path.join(DATA_DIR, 'backup', timestamp)
    os.makedirs(BACKUP_DIR, exist_ok=True)
    
    print("\n" + "=" * 60)
    print("üî• California Wildfire Tweet Sentiment Analysis")
    print("   Âä†Â∑ûÈáéÁÅ´Êé®ÊñáÊÉÖÊÑüÂàÜÊûê")
    print("=" * 60)
    
    # 1. Áà¨ÂèñÊé®Êñá
    tweets = scrape_california_wildfire_tweets(max_tweets=MAX_TWEETS)
    
    if not tweets:
        print("\n‚ùå Êú™Ëé∑ÂèñÂà∞‰ªª‰ΩïÊé®ÊñáÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•ÊàñÁ®çÂêéÈáçËØï")
        return
    
    # ‰øùÂ≠òÂéüÂßãÊï∞ÊçÆÂ§á‰ªΩ
    backup_file = os.path.join(BACKUP_DIR, 'tweets_raw.csv')
    df_backup = pd.DataFrame(tweets)
    df_backup.to_csv(backup_file, index=False)
    print(f"\nüíæ ÂéüÂßãÊï∞ÊçÆÂ∑≤Â§á‰ªΩÂà∞: {backup_file}")
    
    # 2. Â§ÑÁêÜÊé®Êñá
    word_counts, word_sentiments, tweet_data = process_tweets(tweets)
    
    # 3. ÁîüÊàê CSV
    generate_sentiment_csv(word_counts, word_sentiments, OUTPUT_CSV)
    generate_tweets_csv(tweet_data, TWEETS_CSV)
    
    # 4. ÊòæÁ§∫ÁªìÊûú
    print_top_words(word_counts, word_sentiments)
    
    # 5. ÁîüÊàêÊëòË¶ÅÊä•Âëä
    summary = {
        'total_tweets': len(tweets),
        'unique_words': len(word_counts),
        'positive_words': len([w for w in word_sentiments.values() if w == 1]),
        'neutral_words': len([w for w in word_sentiments.values() if w == 0]),
        'negative_words': len([w for w in word_sentiments.values() if w == -1]),
        'generated_at': datetime.now().isoformat(),
        'data_source': 'Twitter via Snscrape',
        'topic': 'California Wildfire'
    }
    
    summary_file = os.path.join(DATA_DIR, 'sentiment_analysis_summary.json')
    with open(summary_file, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("\n" + "=" * 60)
    print("‚úÖ ÂàÜÊûêÂÆåÊàê!")
    print("=" * 60)
    print(f"\nÁîüÊàêÁöÑÊñá‰ª∂:")
    print(f"  1. {OUTPUT_CSV}")
    print(f"  2. {TWEETS_CSV}")
    print(f"  3. {summary_file}")
    print(f"\nüí° Áé∞Âú®ÂèØ‰ª•Â∞Ü sentiment_analysis.csv Â§çÂà∂Âà∞ website/data/ ÁõÆÂΩï")
    print(f"   ÁÑ∂ÂêéÂà∑Êñ∞ vis6 È°µÈù¢Êü•ÁúãËØç‰∫ëÊïàÊûú!")
    print()


if __name__ == "__main__":
    main()
